{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1FHHlht5irB5cAqEOkW0pNM5rB6z-1BOS","timestamp":1675026068162}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X9aRtsE3AMMc","outputId":"ffb02d94-f0be-45e6-c97c-92bdcf0198a1","executionInfo":{"status":"ok","timestamp":1675022471278,"user_tz":-330,"elapsed":18663,"user":{"displayName":"Anish K","userId":"16602377173291074351"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gymnasium in /usr/local/lib/python3.8/dist-packages (0.27.1)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium) (2.2.0)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium) (6.0.0)\n","Requirement already satisfied: gymnasium-notices>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from gymnasium) (0.0.1)\n","Requirement already satisfied: jax-jumpy>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium) (0.2.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium) (4.4.0)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium) (1.21.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gymnasium) (3.11.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gymnasium[classic-control] in /usr/local/lib/python3.8/dist-packages (0.27.1)\n","Requirement already satisfied: gymnasium-notices>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from gymnasium[classic-control]) (0.0.1)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[classic-control]) (2.2.0)\n","Requirement already satisfied: jax-jumpy>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[classic-control]) (0.2.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[classic-control]) (4.4.0)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[classic-control]) (1.21.6)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[classic-control]) (6.0.0)\n","Requirement already satisfied: pygame==2.1.3.dev8 in /usr/local/lib/python3.8/dist-packages (from gymnasium[classic-control]) (2.1.3.dev8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gymnasium[classic-control]) (3.11.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gymnasium[accept-rom-license] in /usr/local/lib/python3.8/dist-packages (0.27.1)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[accept-rom-license]) (6.0.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[accept-rom-license]) (4.4.0)\n","Requirement already satisfied: jax-jumpy>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[accept-rom-license]) (0.2.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[accept-rom-license]) (2.2.0)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[accept-rom-license]) (1.21.6)\n","Requirement already satisfied: gymnasium-notices>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from gymnasium[accept-rom-license]) (0.0.1)\n","Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /usr/local/lib/python3.8/dist-packages (from gymnasium[accept-rom-license]) (0.4.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2.25.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (7.1.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (4.64.1)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (5.10.2)\n","Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (0.5.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gymnasium[accept-rom-license]) (3.11.0)\n","Requirement already satisfied: libtorrent in /usr/local/lib/python3.8/dist-packages (from AutoROM.accept-rom-license->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2.0.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2022.12.7)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (4.0.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gymnasium[atari] in /usr/local/lib/python3.8/dist-packages (0.27.1)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[atari]) (6.0.0)\n","Requirement already satisfied: gymnasium-notices>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from gymnasium[atari]) (0.0.1)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[atari]) (2.2.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[atari]) (4.4.0)\n","Requirement already satisfied: jax-jumpy>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[atari]) (0.2.0)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[atari]) (1.21.6)\n","Requirement already satisfied: shimmy[atari]<1.0,>=0.1.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[atari]) (0.2.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gymnasium[atari]) (3.11.0)\n","Requirement already satisfied: ale-py~=0.8.0 in /usr/local/lib/python3.8/dist-packages (from shimmy[atari]<1.0,>=0.1.0->gymnasium[atari]) (0.8.0)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from ale-py~=0.8.0->shimmy[atari]<1.0,>=0.1.0->gymnasium[atari]) (5.10.2)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: atari-py in /usr/local/lib/python3.8/dist-packages (0.2.9)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from atari-py) (1.21.6)\n","Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from atari-py) (1.15.0)\n","0.27.1\n","Using cpu device\n"]}],"source":["!pip install gymnasium\n","!pip install gymnasium[classic-control]\n","!pip install gymnasium[accept-rom-license]\n","!pip install gymnasium[atari]\n","!pip install atari-py\n","\n","import warnings\n","# warnings.filterwarnings(\"ignore\")\n","\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","\n","import numpy as np\n","import gymnasium as gym\n","print(gym.__version__)\n","from tqdm import tqdm\n","\n","import matplotlib.pyplot as plt\n","import collections\n","import random\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using {device} device\")"]},{"cell_type":"markdown","source":["# **Building DDQN Module**"],"metadata":{"id":"UEuKzT6lA8SF"}},{"cell_type":"code","source":["class Network(nn.Module):\n","    def __init__(self, input_dim, action_space, hidden_layers, activation_fn = nn.ReLU):\n","        super(Network, self).__init__()\n","        layers = [nn.Linear(input_dim, hidden_layers[0]), activation_fn()]\n","        for i in range(len(hidden_layers) - 1):\n","            layers.append(nn.Linear(hidden_layers[i], hidden_layers[i+1]))\n","            layers.append(activation_fn())\n","        \n","        self.sequential = nn.Sequential(*layers)\n","        self.value_output = nn.Linear(hidden_layers[-1], 1)\n","        self.advantage_output = nn.Linear(hidden_layers[-1], action_space)\n","        self.conv1 = nn.Conv2d(3, 6, 5)\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        self.pool = nn.MaxPool2d(2,2)\n","        self.fc = nn.Linear(49*37*16 ,hidden_layers[-1])\n","    \n","    def forward(self, x):\n","        state = x\n","        \n","        if not isinstance(state, torch.Tensor):\n","            state = torch.FloatTensor(state).to(device)\n","            if len(state.size()) < 4 : \n","              state  = np.transpose(state).unsqueeze(0)\n","            else : \n","              state = np.reshape(state, (64, 3, 160, 210))\n","        \n","        #hidden_output = self.sequential(state)\n","        state = self.pool(self.conv1(state))\n","        state = self.pool(self.conv2(state))\n","        state = torch.flatten(state, 1)\n","        state = torch.nn.functional.relu(state)\n","        hidden_output = self.fc(state)\n","        value_fn = self.value_output(hidden_output)\n","        advantage_fn = self.advantage_output(hidden_output)\n","        \n","        value_fn = value_fn.expand(advantage_fn.size())\n","        action_fn = value_fn + advantage_fn - advantage_fn.mean(-1, keepdim=True).expand(advantage_fn.size())\n","        return(action_fn)"],"metadata":{"id":"NgYiAyukCeZ-","executionInfo":{"status":"ok","timestamp":1675022898380,"user_tz":-330,"elapsed":417,"user":{"displayName":"Anish K","userId":"16602377173291074351"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["class DuelingDQN():\n","    def __init__(self, env):\n","        self.env_name = env\n","        self.env = gym.make(env)\n","        self.state = self.env.reset()[0]\n","        self.online_model = Network(len(self.state), self.env.action_space.n, (128, 64)).to(device)\n","        self.target_model = Network(len(self.state), self.env.action_space.n, (128, 64)).to(device)\n","        self.target_model.load_state_dict(self.online_model.state_dict())\n","        \n","        #Hyperparameters\n","        self.epsilon = 0.6\n","        self.min_epsilon = 0.01\n","        self.decay = (self.epsilon - self.min_epsilon) / 15000\n","        self.gamma = 0.95\n","        self.tau = 0.10\n","        self.lr = 1e-4\n","        self.buffer_size = int(1e6)\n","        self.min_buffer_size = 320\n","        self.mini_batch_size = 64\n","        \n","        #Other variables\n","        self.optimizer = optim.Adam(self.online_model.parameters(), self.lr)\n","        self.loss_fn = nn.MSELoss()\n","        self.replay_buffer = collections.deque(maxlen=self.buffer_size)\n","        \n","        self.eval_returns = []\n","    \n","    def optimise(self, experiences):\n","        states = [experience[0] for experience in experiences]\n","        actions = [experience[1] for experience in experiences]\n","        rewards = [experience[2] for experience in experiences]\n","        next_states = [experience[3] for experience in experiences] \n","        is_terminals = [experience[4] for experience in experiences]\n","        \n","        action_values = self.online_model(np.array(states)).squeeze()\n","        idxs = torch.tensor(actions).to(device).long().unsqueeze(1)\n","        action_values = action_values.gather(1, idxs)\n","        \n","        with torch.no_grad():\n","            next_action_values = self.target_model(np.array(next_states)).detach().squeeze()\n","            next_action_values, _ = next_action_values.max(dim=1)\n","            \n","        target = np.array(rewards) + self.gamma*np.array(next_action_values.cpu())*(1-np.array(is_terminals))\n","        \n","        target = torch.from_numpy(target).to(device).unsqueeze(1).float()\n","        \n","        loss = self.loss_fn(action_values, target)\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step() \n","        \n","    def step(self, optimise  = True, update = True):\n","        if random.random() <= self.epsilon:\n","            action = self.env.action_space.sample()\n","        else:\n","            with torch.no_grad():\n","                action_values = self.online_model(self.state).detach()\n","                action = int(torch.argmax(action_values.squeeze()))\n","            \n","        next_state, reward, terminated, truncated , info = self.env.step(action)\n","        \n","        self.epsilon = max(self.epsilon - self.decay, self.min_epsilon)\n","        \n","        self.replay_buffer.append((self.state, action, reward, next_state, terminated and not truncated))\n","        \n","        self.state = next_state\n","        \n","        if terminated or truncated:\n","            self.state = self.env.reset()[0]\n","        \n","        if optimise and len(self.replay_buffer) >= self.min_buffer_size:\n","            self.optimise(random.sample(self.replay_buffer, self.mini_batch_size))\n","        \n","        if update:\n","            for target_param, online_param in zip(self.target_model.parameters(), self.online_model.parameters()):\n","                target_param.data.copy_(self.tau*online_param.data + (1.0-self.tau)*target_param.data)\n","    \n","    def eval_model(self, render):\n","        with torch.no_grad():\n","            eval_env = gym.make(self.env_name)\n","            eval_episodes = 10\n","            for episode in range(eval_episodes):\n","                state = eval_env.reset()[0]\n","                done = False\n","                total_return = 0\n","                while not done:\n","                    action_values = self.online_model(state)\n","                    action = int(torch.argmax(action_values.squeeze()))\n","                    next_state, reward, terminated, truncated , info = eval_env.step(action)\n","                    #if render:\n","                        #eval_env.render(mode = \"human\")\n","                    done = terminated or truncated\n","                    \n","                    total_return += reward\n","                    state = next_state\n","                self.eval_returns.append(total_return)\n","            return np.mean(self.eval_returns[len(self.eval_returns) - eval_episodes: ])\n","    \n","    def plot_results(self):\n","        plt.plot(range(len(self.eval_returns)), self.eval_returns)\n","        plt.xlabel(\"Episode\")\n","        plt.ylabel(\"Score\")\n","        plt.show()\n","                \n","    def train(self, time_stamps, eval_time_stamps, plot_time_stamps):\n","        for time_stamp in tqdm(range(1, time_stamps + 1)):\n","            self.step()\n","            \n","            if(time_stamp % eval_time_stamps) == 0:\n","                if self.eval_model(False) > 480.0:\n","                    print(\"Solved CartPole with final average greater than 480 across 10 episodes\")\n","                    self.plot_results()\n","                    break\n","            \n","            if(time_stamp % plot_time_stamps) == 0:\n","                self.plot_results()"],"metadata":{"id":"EPefY1TgAmyi","executionInfo":{"status":"ok","timestamp":1675022901244,"user_tz":-330,"elapsed":718,"user":{"displayName":"Anish K","userId":"16602377173291074351"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["agent = DuelingDQN(\"Breakout-v4\")\n","agent.train(30000, 250, 3000)"],"metadata":{"id":"knXt80OqCtWI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1e4667e4-1296-48c7-e64d-7a5afda65b2c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["  2%|▏         | 749/30000 [44:30<7:11:24,  1.13it/s]"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Dz0uC2xyC_jE","executionInfo":{"status":"aborted","timestamp":1675022739996,"user_tz":-330,"elapsed":7,"user":{"displayName":"Anish K","userId":"16602377173291074351"}}},"execution_count":null,"outputs":[]}]}